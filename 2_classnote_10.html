<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Class Notes for Design Research</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#数据爬取">数据爬取</a>
<ul>
<li><a href="#功能">功能</a></li>
<li><a href="#输出">输出</a></li>
<li><a href="#使用说明">使用说明</a></li>
</ul>
</li>
<li><a href="#数据分析">数据分析</a>
<ul>
<li></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="数据爬取">数据爬取</h1>
<p><em>来源：<a href="https://github.com/dataabc/weibo-search">https://github.com/dataabc/weibo-search</a></em></p>
<h2 id="功能">功能</h2>
<p>连续获取一个或多个<strong>微博关键词搜索</strong>结果，并将结果写入文件（可选）、数据库（可选）等。所谓微博关键词搜索即：<strong>搜索正文中包含指定关键词的微博</strong>，可以指定搜索的时间范围。<br><br>
举个栗子，比如你可以搜索包含关键词“迪丽热巴”且发布日期在2020-03-01和2020-03-16之间的微博。搜索结果数量巨大，对于非常热门的关键词，在一天的指定时间范围，可以获得<strong>1000万</strong>以上的搜索结果。注意这里的一天指的是时间筛选范围，具体多长时间将这1000万微博下载到本地还要看获取的速度。1000万只是一天时间范围可获取的微博数量，如果想获取更多微博，可以加大时间范围，比如10天，最多可以获得1000万X10=1亿条搜索结果，当然你也可以再加大时间范围。对于大多数关键词，微博一天产生的相关搜索结果应该低于1000万，因此可以说<strong>本程序可以获取指定关键词的全部或近似全部的搜索结果</strong>。本程序可以获得几乎全部的微博信息，如微博正文、发布者等，详情见<a href="#%E8%BE%93%E5%87%BA">输出</a>部分。支持输出多种文件类型，具体如下：</p>
<ul>
<li>写入<strong>csv文件</strong>（默认）</li>
<li>写入<strong>MySQL数据库</strong>（可选）</li>
<li>写入<strong>MongoDB数据库</strong>（可选）</li>
<li>下载微博中的<strong>图片</strong>（可选）</li>
<li>下载微博中的<strong>视频</strong>（可选）</li>
</ul>
<h2 id="输出">输出</h2>
<ul>
<li>微博id：微博的id，为一串数字形式</li>
<li>微博bid：微博的bid</li>
<li>微博内容：微博正文</li>
<li>头条文章url：微博中头条文章的url，若某微博中不存在头条文章，则该值为’’</li>
<li>原始图片url：原创微博图片和转发微博转发理由中图片的url，若某条微博存在多张图片，则每个url以英文逗号分隔，若没有图片则值为’’</li>
<li>视频url: 微博中的视频url和Live Photo中的视频url，若某条微博存在多个视频，则每个url以英文分号分隔，若没有视频则值为’’</li>
<li>微博发布位置：位置微博中的发布位置</li>
<li>微博发布时间：微博发布时的时间，精确到天</li>
<li>点赞数：微博被赞的数量</li>
<li>转发数：微博被转发的数量</li>
<li>评论数：微博被评论的数量</li>
<li>微博发布工具：微博的发布工具，如iPhone客户端、HUAWEI Mate 20 Pro等，若没有则值为’’</li>
<li>话题：微博话题，即两个#中的内容，若存在多个话题，每个url以英文逗号分隔，若没有则值为’’</li>
<li>@用户：微博@的用户，若存在多个@用户，每个url以英文逗号分隔，若没有则值为’’</li>
<li>原始微博id：为转发微博所特有，是转发微博中那条被转发微博的id，那条被转发的微博也会存储，字段和原创微博一样，只是它的本字段为空</li>
<li>结果文件：保存在当前目录“结果文件”文件夹下以关键词为名的文件夹里</li>
<li>微博图片：微博中的图片，保存在以关键词为名的文件夹下的images文件夹里</li>
<li>微博视频：微博中的视频，保存在以关键词为名的文件夹下的videos文件夹里</li>
</ul>
<h2 id="使用说明">使用说明</h2>
<h3 id="下载脚本并解压缩（重要：必须解压缩）">1. 下载脚本并解压缩（重要：必须解压缩）</h3>
<p><a href="https://github.com/dataabc/weibo-search">https://github.com/dataabc/weibo-search</a></p>
<h3 id="安装必要程序">2. 安装必要程序</h3>
<ol>
<li>进入解压缩文件夹，找到“requirements.txt”所在文件夹</li>
<li>在地址栏输入cmd，自动打开窗口<br>
<strong>重要：以后该窗口我们称之为cmd窗口</strong></li>
<li>输入：python --version，确认python版本是3.x</li>
<li>输入：pip --version，确认有反馈信息<br>
<em>如果没有，进行以下步骤：</em><br>
<em>- 找到python所在路径并复制（注意，要在该文件夹下找到Scripts文件夹和python软件才是正确的路径）</em><br>
<em>- 按windows键，搜索高级系统设置</em><br>
<em>- 依次选择：环境变量 —&gt; 系统变量的path —&gt; 编辑</em><br>
<em>- 点击新建并粘贴刚才复制的路径（称为路径1）</em><br>
<em>- 点击新建并粘贴刚才复制的路径，并在最后加上 Sripts\ （称为路径2）</em><br>
<em>- 将路径1移到顶部，路径2移到第二（注意：不能直接拖曳，旁边有上移按钮）</em></li>
<li>输入以下内容，并等待安装完成<pre><code>pip install scrapy
</code></pre>
</li>
<li>输入以下内容，并等待安装完成<pre><code>pip install -r requirements.txt
</code></pre>
</li>
<li>输入以下内容，并等待安装完成<pre><code>pip install scrapy -i https://pypi.douban.com/simple/
</code></pre>
</li>
<li>输入以下内容，并等待安装完成<pre><code>pip install jupyterlab
</code></pre>
</li>
</ol>
<h3 id="设置cookie">3. 设置cookie</h3>
<ol>
<li>进入weibo文件夹，找到 <a href="http://settings.py">settings.py</a>，选择用记事本打开</li>
<li>找到DEFAULT_REQUEST_HEADERS中的cookie并将"your cookie"替换成真实的cookie</li>
<li>如何获取cookie详见 <a href="#%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96cookie">如何获取cookie</a><br>
<strong>重要：cookie包含你微博的登录信息，注意保密，不然你的微博就有被盗风险</strong></li>
</ol>
<h4 id="如何获取cookie">如何获取cookie</h4>
<ol>
<li>用Chrome打开 <a href="https://weibo.com/">https://weibo.com/</a></li>
<li>点击"立即登录", 完成私信验证或手机验证码验证, 进入新版微博. 如下图所示:<br>
<img src="https://user-images.githubusercontent.com/41314224/144813569-cfb5ad32-22f0-4841-afa9-83184b2ccf6f.png" width="400px" alt="..."></li>
<li>右键选择检查打开开发者工具, 筛选XHR文件。在开发者工具的 Network-&gt;Name-&gt; Headers，一个个文件查找，需要在Request Headers下找到"Cookie"。"Cookie:"后的值, 这就是我们要找的cookie值, 复制即可, 如图所示:<br>
<img src="https://picture.cognize.me/cognize/github/weibospider/cookie3.png" width="400px" alt="..."></li>
</ol>
<h3 id="设置搜索关键词">4. 设置搜索关键词</h3>
<p>修改setting.py文件中的KEYWORD_LIST参数。<br>
如果你想搜索一个关键词，如“迪丽热巴”：</p>
<pre><code>KEYWORD_LIST = ['迪丽热巴']
</code></pre>
<p>如果你想分别搜索多个关键词，如想要分别获得“迪丽热巴”和“杨幂”的搜索结果：</p>
<pre><code>KEYWORD_LIST = ['迪丽热巴', '杨幂']
</code></pre>
<p>如果你想搜索同时包含多个关键词的微博，如同时包含“迪丽热巴”和“杨幂”微博的搜索结果：</p>
<pre><code>KEYWORD_LIST = ['迪丽热巴 杨幂']
</code></pre>
<p>如果你想搜索微博话题，即包含#的内容，如“#迪丽热巴#”：</p>
<pre><code>KEYWORD_LIST = ['#迪丽热巴#']
</code></pre>
<p>也可以把关键词写进txt文件里，然后将txt文件路径赋值给KEYWORD_LIST，如：</p>
<pre><code>KEYWORD_LIST = 'keyword_list.txt'
</code></pre>
<p>txt文件中每个关键词占一行。</p>
<h3 id="设置搜索时间范围">5. 设置搜索时间范围</h3>
<p>START_DATE代表搜索的起始日期，END_DATE代表搜索的结束日期，值为“yyyy-mm-dd”形式，程序会搜索包含关键词且发布时间在起始日期和结束日期之间的微博（包含边界）。比如我想筛选发布时间在2020-06-01到2020-06-02这两天的微博：</p>
<pre><code>START_DATE = '2020-06-01'
END_DATE = '2020-06-02'
</code></pre>
<h3 id="其他可选设置">6. 其他可选设置</h3>
<h4 id="设置further_threshold（可选）">设置FURTHER_THRESHOLD（可选）</h4>
<p>FURTHER_THRESHOLD是程序是否进一步搜索的阈值。一般情况下，如果在某个搜索条件下，搜索结果很多，则搜索结果应该有50页微博，多于50页不显示。当总页数等于50时，程序认为搜索结果可能没有显示完全，所以会继续细分。比如，若当前是按天搜索的，程序会把当前的1个搜索分成24个搜索，每个搜索条件粒度是小时。这样就能获取在天粒度下无法获取完全的微博。同理，如果小时粒度下总页数仍然是50，会继续细分，以此类推。然而，有一些关键词，搜索结果即便很多，也只显示40多页。所以此时如果FURTHER_THRESHOLD是50，程序会认为只有这么多微博，不再继续细分，导致很多微博没有获取。因此为了获取更多微博，FURTHER_THRESHOLD应该是小于50的数字。但是如果设置的特别小，如1，这样即便结果真的只有几页，程序也会细分，这些没有必要的细分会使程序速度降低。因此，建议<strong>FURTHER_THRESHOLD的值设置在40与46之间</strong>：</p>
<pre><code>FURTHER_THRESHOLD = 46
</code></pre>
<h4 id="设置结果保存类型（可选）">设置结果保存类型（可选）</h4>
<p>ITEM_PIPELINES是我们可选的结果保存类型，第一个代表去重，第二个代表写入csv文件，第三个代表写入MySQL数据库，第四个代表写入MongDB数据库，第五个代表下载图片，第六个代表下载视频。后面的数字代表执行的顺序，数字越小优先级越高。如果你只要写入部分类型，可以把不需要的类型用“#”注释掉，以节省资源；如果你想写入数据库，需要在setting.py填写相关数据库的配置。</p>
<h4 id="设置等待时间（可选）">设置等待时间（可选）</h4>
<p>DOWNLOAD_DELAY代表访问完一个页面再访问下一个时需要等待的时间，默认为10秒。如我想设置等待15秒左右，可以修改setting.py文件的DOWNLOAD_DELAY参数：</p>
<pre><code>DOWNLOAD_DELAY = 15
</code></pre>
<h4 id="设置微博类型（可选）">设置微博类型（可选）</h4>
<p>WEIBO_TYPE筛选要搜索的微博类型，0代表搜索全部微博，1代表搜索全部原创微博，2代表热门微博，3代表关注人微博，4代表认证用户微博，5代表媒体微博，6代表观点微博。比如我想要搜索全部原创微博，修改setting.py文件的WEIBO_TYPE参数：</p>
<pre><code>WEIBO_TYPE = 1
</code></pre>
<h4 id="设置包含内容（可选）">设置包含内容（可选）</h4>
<p>CONTAIN_TYPE筛选结果微博中必需包含的内容，0代表不筛选，获取全部微博，1代表搜索包含图片的微博，2代表包含视频的微博，3代表包含音乐的微博，4代表包含短链接的微博。比如我想筛选包含图片的微博，修改setting.py文件的CONTAIN_TYPE参数：</p>
<pre><code>CONTAIN_TYPE = 1
</code></pre>
<h4 id="筛选微博发布地区（可选）">筛选微博发布地区（可选）</h4>
<p>REGION筛选微博的发布地区，精确到省或直辖市，值不应包含“省”或“市”等字，如想筛选北京市的微博请用“北京”而不是“北京市”，想要筛选安徽省的微博请用“安徽”而不是“安徽省”，可以写多个地区，具体支持的地名见region.py文件，注意只支持省或直辖市的名字，省下面的市名及直辖市下面的区县名不支持，不筛选请用”全部“。比如我想要筛选发布地在山东省的微博：</p>
<pre><code>REGION = ['山东']
</code></pre>
<h4 id="配置数据库（可选）">配置数据库（可选）</h4>
<p>MONGO_URI是MongoDB数据库的配置；<br><br>
MYSQL开头的是MySQL数据库的配置。</p>
<h3 id="开爬：运行程序">7. 开爬：运行程序</h3>
<p>回到cmd窗口并输入以下内容：</p>
<pre><code>scrapy crawl search -s JOBDIR=crawls/search
</code></pre>
<p>其实只运行“scrapy crawl search”也可以，只是上述方式在结束时可以保存进度，下次运行时会在程序上次的地方继续获取。注意，如果想要保存进度，请使用“Ctrl + C”<strong>一次</strong>，注意是<strong>一次</strong>。按下“Ctrl + C”一次后，程序会继续运行一会，主要用来保存获取的数据、保存进度等操作，请耐心等待。下次再运行时，只要再运行上面的指令就可以恢复上次的进度。</p>
<h3 id="获取结果">8. 获取结果</h3>
<p>爬到的数据会在一个result的文件夹里面。如果还没爬取完就查看，最后先复制一份再打开，不然可能会影响爬取。</p>
<h1 id="数据分析">数据分析</h1>
<p>数据来源与更多学习资料可以参见 LDAsklearn.ipynb 开头的说明</p>
<h3 id="准备工作">准备工作</h3>
<ol>
<li>任意地方新开一个叫“lda”的文件夹</li>
<li>lda文件夹下开三个文件夹，分别叫data、result、stop_dic</li>
<li>将stopwords.txt 和 dict.txt 两个文件下载并放在stop_dic 文件夹</li>
<li>将 <a href="http://lda.py">lda.py</a> 和 LDAsklearn.ipynb 两个文件下载并放在lda文件夹</li>
<li>将爬取结果整理后（详见<a href="#%E7%88%AC%E5%8F%96%E7%BB%93%E6%9E%9C%E6%95%B4%E7%90%86%E6%AD%A5%E9%AA%A4">爬取结果整理步骤</a>），放在data文件夹</li>
</ol>
<h4 id="爬取结果整理步骤">爬取结果整理步骤</h4>
<ul>
<li>删除除微博内容以外的所有列</li>
<li>将微博内容那一列的名字改为 content</li>
<li>文件另存为 data.xlsx</li>
</ul>
<h3 id="设置">设置</h3>
<ol>
<li>打开lda文件夹</li>
<li>在地址栏输入cmd打开cmd窗口，并输入以下内容：<pre><code>jupyter-lab LDAsklearn.ipynb
</code></pre>
</li>
<li>自动弹出浏览器并显示编辑器。如果没有，手动打开浏览器，复制并打开cmd窗口反馈的网址</li>
<li>在编辑器中，找到以下部分，并将所有“你的路径”改成你lda文件夹所在的路径：<pre><code>output_path = '你的路径/lda/result' 
file_path = '你的路径/lda/data' os.chdir(file_path) 
data=pd.read_excel("data.xlsx")#content type os.chdir(output_path) 
dic_file = "你的路径/lda/stop_dic/dict.txt" 
stop_file = "你的路径/lda/stop_dic/stopwords.txt"
</code></pre>
</li>
<li>找到n_topics，修改你要的主题数量（这个参数需要按课上讲的反复调试）</li>
<li>保存文件</li>
</ol>
<h3 id="运行分析">运行分析</h3>
<p>点击kernel，选择restart kernel and run all cells。稍等一会就会看到看是分析（就是代码后面会出现一些新的框框显示结果）。如果没有开始分析，关掉浏览器窗口，关掉所有cmd窗口。重新做 <a href="#%E8%AE%BE%E7%BD%AE">设置</a> 的第一步和第二部，并再次运行分析。</p>
<p>凡是遇到中间出现no module named "xxxxxxx"这类错误，执行以下步骤：</p>
<ol>
<li>打开任意文件夹</li>
<li>地址栏输入cmd</li>
<li>输入并执行以下内容（xxxxxxx需要自己按错误信息替换）：<pre><code>pip install xxxxxxx
</code></pre>
</li>
</ol>
<p><em>Note: 如果需要安装 pyLDAvis，且又报错了。就需要安装Microsoft Visual C++ 14.0以上版本，具体可以看 <a href="https://blog.csdn.net/weixin_40547993/article/details/89399825">https://blog.csdn.net/weixin_40547993/article/details/89399825</a></em></p>

    </div>
  </div>
</body>

</html>
